---
title: "Analysis of liver un-targeted metabolomic data"
author: "mns"
date: "March 11, 2016"
output: pdf_document
---
Data is from a multiple stressors experiment where leopard frogs in water were exposed to either carbaryl, bullfrog stressor, or carbaryl & bullfrog stress. Controls were exposed to the carbaryl carrier only. After a 24 hour acclimation and a 48 hour exposure livers were extracted for metabolomics analysis. Data is in CDF form in Dropbox - \Dropbox\amphib_metabolomics\PIP\data\mns_pip_031116\ and
\Dropbox\amphib_metabolomics\PIP\XCMS_031116_centwave

Libraries used:
```{r}
library(e1071)
library(muma)
```
Data from the GC-MS (autospec) was retention time aligned in xcms online and exported. The exported file had 2665 identified features. Some retention times had more than one m/z value so I sorted the data by median retention time and deleted duplicates based on abunance at a retention time. Here I import that data. 
```{r}
XCMS.diffreport4R <- read.csv("~/git/PIP/GCMS_data_XCMS/XCMS.diffreport4R.csv")
View(XCMS.diffreport4R)
liver1<-XCMS.diffreport4R
```

Here I format the data to look for outliers with a PCA using library(MUMA)
```{r, echo=FALSE}
n<-liver1$rtmed
liver1_t<-as.data.frame(t(liver1[,-1]))
colnames(liver1_t)<-n
View(liver1_t)
liver1_t$Samples <- factor(colnames(liver1)[-1])
# [1] mns13pip3p_again  mns1pip3p      mns20pip2p2      mns20pip4p #     
# [5] mns22pip2p       mns24pip4p       mns26pip1p       mns27pip1p #     
# [9] mns28pip1p2      mns2pip1p2       mns31pip3p2      mns32pip3p #     
#[13] mns34pip3p       mns4pip1p        mns9pip4p        mnsC50pip2p #    
#[17] mnsC51pip4p2     mnsC52pip2p2     mnsC53pip2p2     mnsJ1pip4p #      
#[21] mns12pip4p       mns15pip2p       mns15pip4p2      mns18pip2p  #    
#[25] mns23pip2p       mns23pip4p       mns24pip2p       mns25pip3p #     
#[29] mns26pip3p2      mns30pip1p       mns30pip3p2      mns32pip1p #     
#[33] mns34pip1p       mns35pip1p2      mns3pip3p        mns41pip4p #     
#[37] mns4pip3p2       mns6pip1p        mns9pip2p        mnsC52pip4p       
liver1_t$Class<-c("3","3","1","1","1","1","3","3","3","3","3","3","3","3","1","1","1","1","1","1","2","2","2","2","2","2","2","4","4","4","4","4","4",
                 "4","4","2","4","4","2","2")
head(summary(liver1_t))
liver1_t<-liver1_t[sapply(liver1_t, function(liver1_t)!any(is.na(liver1_t)))]#get rid of columns with NA
data[,c(ncol(data),1:(ncol(data)-1))]
View(liver1_t)
data <- data[,c(1:6,18,7:17)] 
liver2_t<-liver1_t[,c(1587, 2:1586)]
View(liver2_t)
setwd("~/git/PIP/GCMS_data_XCMS")
write.csv(liver2_t, file="liver2_t_out.csv")
```

MUMA for outlier analysis w 4 treatment groups
```{r}
setwd("~/git/PIP/GCMS_data_XCMS")
work.dir(dir.name="WorkDir_liver1")
# delete first row in excel of .csv 

explore.data(file="liver2_t_out.csv", scaling="a", scal=TRUE, normalize=TRUE, imputation=FALSE, imput="ImputType")
par( mfrow = c( 1, 2 ) )
Plot.pca(pcx=1, pcy=2, scaling="a", test.outlier=TRUE)
explore.data(file="liver2_t_out.csv", scaling="p", scal=TRUE, normalize=TRUE, imputation=FALSE, imput="ImputType")
```
No obvious outliers were present.

PLS-DA plots with all data at two different scaling - pareto and auto scaling.
```{r}
plsda(scaling="a") 
Plot.plsda(pcx=1, pcy=2, scaling="a")
Plot.plsda(pcx=2, pcy=3, scaling="a")

plsda(scaling="p") 
Plot.plsda(pcx=1, pcy=2, scaling="a")
Plot.plsda(pcx=2, pcy=3, scaling="a")
```
Black- control
Red - pesticide only
Green - bullfrog only
Blue - pesticide + bullfrog

The PLS-DA distinguishes the pesticide only treatment from the other treatments in component 1 (29.2%) the most readily w either pareto or autoscaling options. The other treatments were mostly overlapping in pls-da space so I decided to eliminate the fourth treatment (pesticide + bullfrog) to see if the pls-da can better distinguish individual effects.

```{r}
#set up directory structure for MUMA pca and plsda
setwd("~/git/PIP/GCMS_data_XCMS")
work.dir(dir.name="WorkDir_liver2")
#pca
explore.data(file="liver3_t_out.csv", scaling="a", scal=TRUE, normalize=TRUE, imputation=FALSE, imput="ImputType")
par( mfrow = c( 1, 2 ) )
Plot.pca(pcx=1, pcy=2, scaling="p", test.outlier=TRUE)
explore.data(file="liver3_t_out.csv", scaling="p", scal=TRUE, normalize=TRUE, imputation=FALSE, imput="ImputType")
#plsda
plsda(scaling="p") 
Plot.plsda(pcx=1, pcy=2, scaling="p")
plsda(scaling="a") 
Plot.plsda(pcx=1, pcy=2, scaling="a")
```

Support Vector Machine data analysis. 
                  
```{r}
# prepare data for SVM control vs. pesticide
class <- read.csv("~/git/PIP/GCMS_data_XCMS/WorkDir_liver1/Preprocessing_Data_a/class.csv")
ProcessedTable <- read.csv("~/git/PIP/GCMS_data_XCMS/WorkDir_liver1/Preprocessing_Data_a/ProcessedTable.csv")
allliver<-cbind(class$V1, ProcessedTable)
View(allliver)
allliver2<-allliver[order(class$V1),]
View(allliver2)
liver1_2<-allliver2[-21:-40,]
View(liver1_2)
liver1_2$Class<-as.factor(liver1_2$`class$V1`)
set.seed(34)
#test tuning
gamma<-c(0.1,1)
tuned <- tune.svm(as.factor(Class)~., data = liver1_2, gamma = gamma, cost = 0.1, tunecontrol = tune.control(cross = 3), scale=FALSE)
#tuning across a larger range
cost.vector<-c(1e-10,1e-5,1e-1,1e1,1e5,1e10)
gamma.vector<-c(1e-10,1e-5,1e-1,1e1,1e5,1e10)
accuracy.vector<-NULL
m9<-matrix(NA, length(cost.vector), length(gamma.vector))

for (k in seq_along(cost.vector)){
  for (j in seq_along(gamma.vector)){    
    for (i in 1:10){
      model.radial.all <- svm(as.factor(Class)~., data=liver1_2, kernal="radial", gamma=gamma.vector[j], cost=cost.vector[k], scale=FALSE, cross=3)
      accuracy.vector[i]<-model.radial.all$tot.accuracy
    }
    m9[k, j]<-mean(accuracy.vector)  
  }
}
m9
 [,1]  [,2]  [,3]  [,4]  [,5]  [,6]
[1,] 36.62 36.58 36.78 36.56 36.54 36.66
[2,] 36.68 36.68 36.54 36.64 36.58 36.58
[3,] 36.56 36.76 36.72 36.64 36.62 36.58
[4,] 36.58 36.76 36.54 36.62 36.54 36.58
[5,] 36.74 37.42 36.46 36.66 36.48 36.66
[6,] 37.38 37.52 36.56 36.52 36.66 36.50
```

RFE code
```{r}
#### other R implementation of RFE for SVM ####
svmrfeFeatureRanking = function(x,y){
  n = ncol(x)
  survivingFeaturesIndexes = seq(1:n)
  featureRankedList = vector(length=n)
  rankedFeatureIndex = n
  while(length(survivingFeaturesIndexes)>0){
    #train the support vector machine
    svmModel = svm(x[, survivingFeaturesIndexes], y, cost = 6.4e9, gamma=1.0e-10, cachesize=500,
                   scale=F, type="C-classification", kernel="radial" )
    #compute the weight vector
    w = t(svmModel$coefs)%*%svmModel$SV
    #compute ranking criteria
    rankingCriteria = w * w
    #rank the features
    ranking = sort(rankingCriteria, index.return = TRUE)$ix
    #update feature ranked list
    featureRankedList[rankedFeatureIndex] = survivingFeaturesIndexes[ranking[1]]
    rankedFeatureIndex = rankedFeatureIndex - 1
    #eliminate the feature with smallest ranking criterion
    (survivingFeaturesIndexes = survivingFeaturesIndexes[-ranking[1]])
  }
  return (featureRankedList)
}
```
RFE and SVM with RFE
```{r}
#prepare data for RFE
X<-liver1_2[,-1:-2]
X<-X[,-1585]
Y<-liver1_2$Class
#RFE
featureRankedList <-svmrfeFeatureRanking(X,Y)
#SVM w/ RFE
svmModel = svm(X[, featureRankedList[1:100]], Y, cost = 1e10, gamma=1.0e-10, kernel="radial", cross=3 )
svmModel 
summary(svmModel)
print(svmModel)
```

```{r}
#### do 250 bootstrapped leave one out cross validation for SVM with 1-1500 feature ranked bins ####
no.features<-seq(1,1500,by=100)
out.acc<-NULL
for (j in seq_along(no.features)){
  for (i in 1:250){
    svmModel3 = svm(X[, featureRankedList[1:no.features[j]]], Y, cost = 1e10, kernel="radial", cross=3 )
    accuracy.vector[i]<-svmModel3$tot.accuracy
  }
  out.acc[j]<-mean(accuracy.vector)  
}
out.acc
acc4merge<-as.data.frame(cbind(no.features, out.acc))
View(acc4merge)
#plot of output
plot(acc4merge$no.features, acc4merge$out.acc, ylab="% Accuracy", xlab="Number of bins")
# between 0 and 200 bins the accuracy increases to >90% and then decreases after 200 bins

#more fine scale to determine where the highest accuracy is per bin
no.features<-seq(1,200,by=10)
out.acc<-NULL
for (j in seq_along(no.features)){
  for (i in 1:250){
    svmModel3 = svm(X[, featureRankedList[1:no.features[j]]], Y, cost = 1e10, kernel="radial", cross=3 )
    accuracy.vector[i]<-svmModel3$tot.accuracy
  }
  out.acc[j]<-mean(accuracy.vector)  
}
out.acc
acc4merge<-as.data.frame(cbind(no.features, out.acc))
View(acc4merge)
plot(acc4merge$no.features, acc4merge$out.acc, ylab="% Accuracy", xlab="Number of bins")
```
The number of feature ranked bins to include to get the highest accuracy is ~70 bins for control vs. pesticide only. 

Combine the feature ranked list with the features for control vs. pesticide
```{r}
library(R.utils)
liver1_2t<-t(liver1_2)
View(liver1_2t)
#inserted 2 NAs in beginning 
featureRankedList4join <- insert(featureRankedList, ats=1, values=rep(NA,2))
featureRankedList4join2 <- insert(featureRankedList4join, ats=1587, values=(NA))
View(featureRankedList4join2)
dim(liver1_2t)
liver1_2twRFE<-cbind(featureRankedList4join2, liver1_2t)
View(liver1_2twRFE)
write.csv(liver1_2twRFE, "RFE_ctrlvspest.csv")
```
This next section's function is to do the same thing for the control vs. bullfrog only exposed leopard frogs. 

```{r}
View(allliver2)
liver1_3<-allliver2[which(allliver2$'class$V1'=='1' | allliver2$'class$V1'=='3'), ]
View(liver1_3)
liver1_3$Class<-as.factor(liver1_3$`class$V1`)
set.seed(34)
#test tuning
gamma<-c(0.1,1)
#test tuning to see how long it takes
tuned <- tune.svm(as.factor(Class)~., data = liver1_3, gamma = gamma, cost = 0.1, tunecontrol = tune.control(cross = 3), scale=FALSE)
#tuning across a larger range
cost.vector<-c(1e-10,1e-5,1e-1,1e1,1e5,1e10)
gamma.vector<-c(1e-10,1e-5,1e-1,1e1,1e5,1e10)
accuracy.vector<-NULL
m9<-matrix(NA, length(cost.vector), length(gamma.vector))

for (k in seq_along(cost.vector)){
  for (j in seq_along(gamma.vector)){    
    for (i in 1:10){
      model.radial.all <- svm(as.factor(Class)~., data=liver1_3, kernal="radial", gamma=gamma.vector[j], cost=cost.vector[k], scale=FALSE, cross=3)
      accuracy.vector[i]<-model.radial.all$tot.accuracy
    }
    m9[k, j]<-mean(accuracy.vector)  
  }
}
m9
 [,1] [,2] [,3] [,4] [,5] [,6]
[1,] 36.5 32.0 35.5 36.0 37.5 40.0
[2,] 35.0 35.5 38.0 32.0 37.5 35.5
[3,] 33.5 36.0 35.5 34.5 38.0 34.5
[4,] 40.0 37.0 38.0 37.0 40.5 38.0
[5,] 37.5 43.0 35.5 38.0 38.0 41.5
[6,] 42.5 50.5 39.5 37.0 36.0 41.0
```

RFE and SVM with RFE for control vs. bullfrog only. The RFE function has already been created above. 
```{r}
#prepare data for RFE
X<-liver1_3[,-1:-2]
X<-X[,-1585]
Y<-liver1_3$Class
#RFE
featureRankedList <-svmrfeFeatureRanking(X,Y)
#SVM w/ RFE
svmModel = svm(X[, featureRankedList[1:200]], Y, cost = 1e10, gamma=1.0e-10, kernel="radial", cross=3 )
svmModel 
summary(svmModel)
print(svmModel)

#### do 250 bootstrapped leave one out cross validation for SVM with 1-1500 feature ranked bins ####
no.features<-seq(1,1500,by=100)
out.acc<-NULL
for (j in seq_along(no.features)){
  for (i in 1:250){
    svmModel3 = svm(X[, featureRankedList[1:no.features[j]]], Y, cost = 1e10, kernel="radial", cross=3 )
    accuracy.vector[i]<-svmModel3$tot.accuracy
  }
  out.acc[j]<-mean(accuracy.vector)  
}
out.acc
acc4merge<-as.data.frame(cbind(no.features, out.acc))
View(acc4merge)
#plot of output
plot(acc4merge$no.features, acc4merge$out.acc, ylab="% Accuracy", xlab="Number of bins")
```
