---
title: "Analysis of liver un-targeted metabolomic data"
author: "mns"
date: "March 11, 2016"
output: pdf_document
---
Data is from a multiple stressors experiment where leopard frogs in water were exposed to either carbaryl, bullfrog stressor, or carbaryl & bullfrog stress. Controls were exposed to the carbaryl carrier only. After a 24 hour acclimation and a 48 hour exposure livers were extracted for metabolomics analysis. Data is in CDF form in Dropbox - \Dropbox\amphib_metabolomics\PIP\data\mns_pip_031116\ and
\Dropbox\amphib_metabolomics\PIP\XCMS_031116_centwave

Libraries used:
```{r}
library(e1071)
library(muma)
```
Data from the GC-MS (autospec) was retention time aligned in xcms online and exported. The exported file had 2665 identified features. Some retention times had more than one m/z value so I sorted the data by median retention time and deleted duplicates based on abunance at a retention time. Here I import that data. 
```{r}
XCMS.diffreport4R <- read.csv("~/git/PIP/GCMS_data_XCMS/XCMS.diffreport4R.csv")
View(XCMS.diffreport4R)
liver1<-XCMS.diffreport4R
```

Here I format the data to look for outliers with a PCA using library(MUMA)
```{r, echo=FALSE}
n<-liver1$rtmed
liver1_t<-as.data.frame(t(liver1[,-1]))
colnames(liver1_t)<-n
View(liver1_t)
liver1_t$Samples <- factor(colnames(liver1)[-1])
# [1] mns13pip3p_again  mns1pip3p      mns20pip2p2      mns20pip4p #     
# [5] mns22pip2p       mns24pip4p       mns26pip1p       mns27pip1p #     
# [9] mns28pip1p2      mns2pip1p2       mns31pip3p2      mns32pip3p #     
#[13] mns34pip3p       mns4pip1p        mns9pip4p        mnsC50pip2p #    
#[17] mnsC51pip4p2     mnsC52pip2p2     mnsC53pip2p2     mnsJ1pip4p #      
#[21] mns12pip4p       mns15pip2p       mns15pip4p2      mns18pip2p  #    
#[25] mns23pip2p       mns23pip4p       mns24pip2p       mns25pip3p #     
#[29] mns26pip3p2      mns30pip1p       mns30pip3p2      mns32pip1p #     
#[33] mns34pip1p       mns35pip1p2      mns3pip3p        mns41pip4p #     
#[37] mns4pip3p2       mns6pip1p        mns9pip2p        mnsC52pip4p       
liver1_t$Class<-c("3","3","1","1","1","1","3","3","3","3","3","3","3","3","1","1","1","1","1","1","2","2","2","2","2","2","2","4","4","4","4","4","4",
                 "4","4","2","4","4","2","2")
head(summary(liver1_t))
liver1_t<-liver1_t[sapply(liver1_t, function(liver1_t)!any(is.na(liver1_t)))]#get rid of columns with NA
data[,c(ncol(data),1:(ncol(data)-1))]
View(liver1_t)
data <- data[,c(1:6,18,7:17)] 
liver2_t<-liver1_t[,c(1587, 2:1586)]
View(liver2_t)
setwd("~/git/PIP/GCMS_data_XCMS")
write.csv(liver2_t, file="liver2_t_out.csv")
```

MUMA for outlier analysis w 4 treatment groups
```{r}
setwd("~/git/PIP/GCMS_data_XCMS")
work.dir(dir.name="WorkDir_liver1")
# delete first row in excel of .csv 

explore.data(file="liver2_t_out.csv", scaling="a", scal=TRUE, normalize=TRUE, imputation=FALSE, imput="ImputType")
par( mfrow = c( 1, 2 ) )
Plot.pca(pcx=1, pcy=2, scaling="a", test.outlier=TRUE)
explore.data(file="liver2_t_out.csv", scaling="p", scal=TRUE, normalize=TRUE, imputation=FALSE, imput="ImputType")
```
No obvious outliers were present.

PLS-DA plots with all data at two different scaling - pareto and auto scaling.
```{r}
plsda(scaling="a") 
Plot.plsda(pcx=1, pcy=2, scaling="a")
Plot.plsda(pcx=2, pcy=3, scaling="a")

plsda(scaling="p") 
Plot.plsda(pcx=1, pcy=2, scaling="a")
Plot.plsda(pcx=2, pcy=3, scaling="a")
```
Black- control
Red - pesticide only
Green - bullfrog only
Blue - pesticide + bullfrog

The PLS-DA distinguishes the pesticide only treatment from the other treatments in component 1 (29.2%) the most readily w either pareto or autoscaling options. The other treatments were mostly overlapping in pls-da space so I decided to eliminate the fourth treatment (pesticide + bullfrog) to see if the pls-da can better distinguish individual effects.

```{r}
#set up directory structure for MUMA pca and plsda
setwd("~/git/PIP/GCMS_data_XCMS")
work.dir(dir.name="WorkDir_liver2")
#pca
explore.data(file="liver3_t_out.csv", scaling="a", scal=TRUE, normalize=TRUE, imputation=FALSE, imput="ImputType")
par( mfrow = c( 1, 2 ) )
Plot.pca(pcx=1, pcy=2, scaling="p", test.outlier=TRUE)
explore.data(file="liver3_t_out.csv", scaling="p", scal=TRUE, normalize=TRUE, imputation=FALSE, imput="ImputType")
#plsda
plsda(scaling="p") 
Plot.plsda(pcx=1, pcy=2, scaling="p")
plsda(scaling="a") 
Plot.plsda(pcx=1, pcy=2, scaling="a")
```

Support Vector Machine data analysis. 
                  
```{r}
# prepare data for SVM control vs. pesticide
class <- read.csv("~/git/PIP/GCMS_data_XCMS/WorkDir_liver1/Preprocessing_Data_a/class.csv")
ProcessedTable <- read.csv("~/git/PIP/GCMS_data_XCMS/WorkDir_liver1/Preprocessing_Data_a/ProcessedTable.csv")
allliver<-cbind(class$V1, ProcessedTable)
View(allliver)
allliver2<-allliver[order(class$V1),]
View(allliver2)
liver1_2<-allliver2[-21:-40,]
View(liver1_2)
liver1_2$Class<-as.factor(liver1_2$`class$V1`)
set.seed(34)
#test tuning
gamma<-c(0.1,1)
tuned <- tune.svm(as.factor(Class)~., data = liver1_2, gamma = gamma, cost = 0.1, tunecontrol = tune.control(cross = 3), scale=FALSE)
#tuning across a larger range
cost.vector<-c(1e-10,1e-5,1e-1,1e1,1e5,1e10)
gamma.vector<-c(1e-10,1e-5,1e-1,1e1,1e5,1e10)
accuracy.vector<-NULL
m9<-matrix(NA, length(cost.vector), length(gamma.vector))

for (k in seq_along(cost.vector)){
  for (j in seq_along(gamma.vector)){    
    for (i in 1:10){
      model.radial.all <- svm(as.factor(Class)~., data=liver1_2, kernal="radial", gamma=gamma.vector[j], cost=cost.vector[k], scale=FALSE, cross=3)
      accuracy.vector[i]<-model.radial.all$tot.accuracy
    }
    m9[k, j]<-mean(accuracy.vector)  
  }
}
m9
 [,1]  [,2]  [,3]  [,4]  [,5]  [,6]
[1,] 36.62 36.58 36.78 36.56 36.54 36.66
[2,] 36.68 36.68 36.54 36.64 36.58 36.58
[3,] 36.56 36.76 36.72 36.64 36.62 36.58
[4,] 36.58 36.76 36.54 36.62 36.54 36.58
[5,] 36.74 37.42 36.46 36.66 36.48 36.66
[6,] 37.38 37.52 36.56 36.52 36.66 36.50
```

RFE code
```{r}
#### other R implementation of RFE for SVM ####
svmrfeFeatureRanking = function(x,y){
  n = ncol(x)
  survivingFeaturesIndexes = seq(1:n)
  featureRankedList = vector(length=n)
  rankedFeatureIndex = n
  while(length(survivingFeaturesIndexes)>0){
    #train the support vector machine
    svmModel = svm(x[, survivingFeaturesIndexes], y, cost = 6.4e9, gamma=1.0e-10, cachesize=500,
                   scale=F, type="C-classification", kernel="radial" )
    #compute the weight vector
    w = t(svmModel$coefs)%*%svmModel$SV
    #compute ranking criteria
    rankingCriteria = w * w
    #rank the features
    ranking = sort(rankingCriteria, index.return = TRUE)$ix
    #update feature ranked list
    featureRankedList[rankedFeatureIndex] = survivingFeaturesIndexes[ranking[1]]
    rankedFeatureIndex = rankedFeatureIndex - 1
    #eliminate the feature with smallest ranking criterion
    (survivingFeaturesIndexes = survivingFeaturesIndexes[-ranking[1]])
  }
  return (featureRankedList)
}
```
RFE and SVM with RFE for control vs. pesticide
```{r}
#prepare data for RFE
X<-liver1_2[,-1:-2]
X<-X[,-1585]
Y<-liver1_2$Class
#RFE
featureRankedList <-svmrfeFeatureRanking(X,Y)
#SVM w/ RFE
svmModel = svm(X[, featureRankedList[1:100]], Y, cost = 1e10, gamma=1.0e-10, kernel="radial", cross=3 )
svmModel 
summary(svmModel)
print(svmModel)
```
Bootstrapped accuracy for control vs. pesticide w/ rfe
```{r}
#### do 250 bootstrapped leave one out cross validation for SVM with 1-1500 feature ranked bins ####
no.features<-seq(1,1500,by=100)
out.acc<-NULL
for (j in seq_along(no.features)){
  for (i in 1:250){
    svmModel3 = svm(X[, featureRankedList[1:no.features[j]]], Y, cost = 1e10, kernel="radial", cross=3 )
    accuracy.vector[i]<-svmModel3$tot.accuracy
  }
  out.acc[j]<-mean(accuracy.vector)  
}
out.acc
acc4merge<-as.data.frame(cbind(no.features, out.acc))
View(acc4merge)
#plot of output
plot(acc4merge$no.features, acc4merge$out.acc, ylab="% Accuracy", xlab="Number of bins", main="control vs. pesticide")
# between 0 and 200 bins the accuracy increases to >90% and then decreases after 200 bins

#more fine scale to determine where the highest accuracy is per bin
no.features<-seq(1,200,by=10)
out.acc<-NULL
for (j in seq_along(no.features)){
  for (i in 1:250){
    svmModel3 = svm(X[, featureRankedList[1:no.features[j]]], Y, cost = 1e10, kernel="radial", cross=3 )
    accuracy.vector[i]<-svmModel3$tot.accuracy
  }
  out.acc[j]<-mean(accuracy.vector)  
}
out.acc
acc4merge<-as.data.frame(cbind(no.features, out.acc))
View(acc4merge)
plot(acc4merge$no.features, acc4merge$out.acc, ylab="% Accuracy", xlab="Number of bins")
```
The number of feature ranked bins to include to get the highest accuracy is ~70 bins for control vs. pesticide only. 

Combine the feature ranked list with the features for control vs. pesticide
```{r}
library(R.utils)
liver1_2t<-t(liver1_2)
View(liver1_2t)
#inserted 2 NAs in beginning 
featureRankedList4join <- insert(featureRankedList, ats=1, values=rep(NA,2))
featureRankedList4join2 <- insert(featureRankedList4join, ats=1587, values=(NA))
View(featureRankedList4join2)
dim(liver1_2t)
liver1_2twRFE<-cbind(featureRankedList4join2, liver1_2t)
View(liver1_2twRFE)
write.csv(liver1_2twRFE, "RFE_ctrlvspest.csv")
```
This next section's function is to do the same thing for the control vs. bullfrog only exposed leopard frogs. 

```{r}
View(allliver2)
liver1_3<-allliver2[which(allliver2$'class$V1'=='1' | allliver2$'class$V1'=='3'), ]
View(liver1_3)
liver1_3$Class<-as.factor(liver1_3$`class$V1`)
set.seed(34)
#test tuning
gamma<-c(0.1,1)
#test tuning to see how long it takes
tuned <- tune.svm(as.factor(Class)~., data = liver1_3, gamma = gamma, cost = 0.1, tunecontrol = tune.control(cross = 3), scale=FALSE)
#tuning across a larger range
cost.vector<-c(1e-10,1e-5,1e-1,1e1,1e5,1e10)
gamma.vector<-c(1e-10,1e-5,1e-1,1e1,1e5,1e10)
accuracy.vector<-NULL
m9<-matrix(NA, length(cost.vector), length(gamma.vector))

for (k in seq_along(cost.vector)){
  for (j in seq_along(gamma.vector)){    
    for (i in 1:10){
      model.radial.all <- svm(as.factor(Class)~., data=liver1_3, kernal="radial", gamma=gamma.vector[j], cost=cost.vector[k], scale=FALSE, cross=3)
      accuracy.vector[i]<-model.radial.all$tot.accuracy
    }
    m9[k, j]<-mean(accuracy.vector)  
  }
}
m9
 [,1] [,2] [,3] [,4] [,5] [,6]
[1,] 36.5 32.0 35.5 36.0 37.5 40.0
[2,] 35.0 35.5 38.0 32.0 37.5 35.5
[3,] 33.5 36.0 35.5 34.5 38.0 34.5
[4,] 40.0 37.0 38.0 37.0 40.5 38.0
[5,] 37.5 43.0 35.5 38.0 38.0 41.5
[6,] 42.5 50.5 39.5 37.0 36.0 41.0
```

RFE code for control vs. bullfrog
```{r}
#### other R implementation of RFE for SVM ####
svmrfeFeatureRanking = function(x,y){
  n = ncol(x)
  survivingFeaturesIndexes = seq(1:n)
  featureRankedList = vector(length=n)
  rankedFeatureIndex = n
  while(length(survivingFeaturesIndexes)>0){
    #train the support vector machine
    svmModel = svm(x[, survivingFeaturesIndexes], y, cost = 1e10, gamma=1.0e-5, cachesize=500,
                   scale=F, type="C-classification", kernel="radial" )
    #compute the weight vector
    w = t(svmModel$coefs)%*%svmModel$SV
    #compute ranking criteria
    rankingCriteria = w * w
    #rank the features
    ranking = sort(rankingCriteria, index.return = TRUE)$ix
    #update feature ranked list
    featureRankedList[rankedFeatureIndex] = survivingFeaturesIndexes[ranking[1]]
    rankedFeatureIndex = rankedFeatureIndex - 1
    #eliminate the feature with smallest ranking criterion
    (survivingFeaturesIndexes = survivingFeaturesIndexes[-ranking[1]])
  }
  return (featureRankedList)
}
```

RFE and SVM with RFE for control vs. bullfrog only. The RFE function has already been created above. 
```{r}
#prepare data for RFE
X<-liver1_3[,-1:-2]
X<-X[,-1585]
Y<-liver1_3$Class
#RFE
featureRankedList <-svmrfeFeatureRanking(X,Y)
#SVM w/ RFE
svmModel = svm(X[, featureRankedList[1:200]], Y, cost = 1e10, gamma=1.0e-5, kernel="radial", cross=3 )
svmModel 
summary(svmModel)
print(svmModel)

#### do 250 bootstrapped leave one out cross validation for SVM with 1-1500 feature ranked bins ####
no.features<-seq(1,1500,by=100)
out.acc<-NULL
for (j in seq_along(no.features)){
  for (i in 1:100){
    svmModel3 = svm(X[, featureRankedList[1:no.features[j]]], Y, cost = 1e10, kernel="radial", cross=3 )
    accuracy.vector[i]<-svmModel3$tot.accuracy
  }
  out.acc[j]<-mean(accuracy.vector)  
}
out.acc
acc4merge<-as.data.frame(cbind(no.features, out.acc))
View(acc4merge)
#plot of output
plot(acc4merge$no.features, acc4merge$out.acc, ylab="% Accuracy", xlab="Number of bins", main="control vs. bullfrog")
#finer scale plot
no.features<-seq(1,200,by=20)
out.acc<-NULL
for (j in seq_along(no.features)){
  for (i in 1:100){
    svmModel3 = svm(X[, featureRankedList[1:no.features[j]]], Y, cost = 1e10, kernel="radial", cross=3 )
    accuracy.vector[i]<-svmModel3$tot.accuracy
  }
  out.acc[j]<-mean(accuracy.vector)  
}
out.acc
acc4merge<-as.data.frame(cbind(no.features, out.acc))
View(acc4merge)
#plot of output
plot(acc4merge$no.features, acc4merge$out.acc, ylab="% Accuracy", xlab="Number of bins", main="control vs. bullfrog")
```

Combine the feature ranked list with the features for control vs. bullfrog
```{r}
library(R.utils)
liver1_3t<-t(liver1_3)
View(liver1_3t)
#inserted 2 NAs in beginning 
featureRankedList4join <- insert(featureRankedList, ats=1, values=rep(NA,2))
featureRankedList4join2 <- insert(featureRankedList4join, ats=1587, values=(NA))
View(featureRankedList4join2)
dim(liver1_3t)
liver1_3twRFE<-cbind(featureRankedList4join2, liver1_3t)
View(liver1_3twRFE)
setwd("~/git/PIP/GCMS_data_XCMS/output")
write.csv(liver1_3twRFE, "RFE_ctrlvbullfrog.csv")
```

Create data with just the subset (n=75) of feature ranked bins used to classify pesticide vs. control with >90% accuracy
```{r}
View(allliver2) #allliver2 has treatments, sample ID, and metabolites
liver1_4<-allliver2[which(allliver2$'class$V1'=='1' | allliver2$'class$V1'=='4'), ]
View(liver1_4)
liver1_4$Class<-as.factor(liver1_4$`class$V1`)
liver1_4.t<-t(liver1_4)
View(liver1_4.t)
dim(liver1_4.t)
featureRankedList4join1_4 <- insert(featureRankedList, ats=1,values=rep(NA,2))
featureRankedList4join1_4 <- insert(featureRankedList4join1_4, ats=1587,values=(NA))
yuck<-cbind(featureRankedList4join1_4,liver1_4.t )
View(yuck)
yuck<-as.data.frame(yuck)
yuck$featureRankedList4join1_4<- as.numeric(as.character(yuck$featureRankedList4join1_4))
yuck2<-yuck[which(yuck$featureRankedList4join1_4 < 76) ,]
View(yuck2)
yuck3<-yuck2[,-1]
View(yuck3)
yuck4<-t(yuck3)
#convert factors to numeric
yuck4[ ] <- lapply(yuck4, function(x) {
    if(is.factor(x)) as.numeric(as.character(x)) else x
})
sapply(yuck4, class)
summary(yuck4)
yuck4$Class<-liver1_4$Class
```

SVM of control vs. pesticide + bullfrog with only 75 top ranked bins from control vs pesticide

```{r}
#make sure tune works
tuned <- tune.svm(as.factor(Class)~., data = yuck4, gamma = 1.0, cost = 0.1, tunecontrol = tune.control(cross = 3), scale=FALSE)
#tune first
cost.vector<-c(1e-10,1e-5,1e-1,1e0, 1e1,1e5,1e10)
gamma.vector<-c(1e-10,1e-5,1e-1,1e0, 1e1,1e5,1e10)
accuracy.vector<-NULL
m9<-matrix(NA, length(cost.vector), length(gamma.vector))

for (k in seq_along(cost.vector)){
  for (j in seq_along(gamma.vector)){    
    for (i in 1:100){
      model.radial.all <- svm(as.factor(Class)~., data=yuck4, kernal="radial", gamma=gamma.vector[j], cost=cost.vector[k], scale=FALSE, cross=3)
      accuracy.vector[i]<-model.radial.all$tot.accuracy
    }
    m9[k, j]<-mean(accuracy.vector)  
  }
}
m9
 [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]
[1,] 37.30 38.20 37.05 37.00 37.95 36.30 36.40
[2,] 35.95 36.75 36.25 37.60 36.95 37.35 36.45
[3,] 38.65 36.85 36.85 35.80 38.60 36.70 38.55
[4,] 36.40 36.50 35.85 37.30 38.90 36.75 37.80
[5,] 36.70 36.80 36.95 37.30 38.10 36.90 36.20
[6,] 35.50 51.50 37.25 37.35 37.55 37.75 37.80
[7,] 48.45 49.90 36.25 36.50 37.50 36.30 37.05

tuned <- tune.svm(as.factor(Class)~., data = yuck4, gamma = 1.0, cost = 0.1, tunecontrol = tune.control(cross = 3), scale=FALSE)
#tune first
cost.vector<-seq(from=1e10, to=1e11, length.out=6)
gamma.vector<-seq(1e-11, 1e-10,length.out=6)
accuracy.vector<-NULL
m9<-matrix(NA, length(cost.vector), length(gamma.vector))

for (k in seq_along(cost.vector)){
  for (j in seq_along(gamma.vector)){    
    for (i in 1:200){
      model.radial.all <- svm(as.factor(Class)~., data=yuck4, kernal="radial", gamma=gamma.vector[j], cost=cost.vector[k], scale=FALSE, cross=3)
      accuracy.vector[i]<-model.radial.all$tot.accuracy
    }
    m9[k, j]<-mean(accuracy.vector)  
  }
}
m9
[,1]   [,2]   [,3]   [,4]   [,5]   [,6]
[1,] 39.300 46.275 47.775 49.825 49.075 48.850
[2,] 46.175 48.625 50.575 49.925 51.500 50.150
[3,] 47.875 51.150 50.125 50.450 50.275 50.650
[4,] 48.975 50.200 50.575 50.750 49.375 49.125
[5,] 47.275 50.550 49.700 49.750 51.475 49.825
[6,] 50.425 49.400 51.400 50.275 50.725 49.750

svmModel = svm(X[, featureRankedList[1:200]], Y, cost = 1e10, gamma=1.0e-10, kernel="radial", cross=3 )
```
RFE tuning optimized for control vs. pest+bullfrog 75 bins (from pesticide vs. control)
```{r}
#### other R implementation of RFE for SVM ####
svmrfeFeatureRanking = function(x,y){
  n = ncol(x)
  survivingFeaturesIndexes = seq(1:n)
  featureRankedList = vector(length=n)
  rankedFeatureIndex = n
  while(length(survivingFeaturesIndexes)>0){
    #train the support vector machine
    svmModel = svm(x[, survivingFeaturesIndexes], y, cost = 1e5, gamma=1.0e-5, cachesize=500,
                   scale=F, type="C-classification", kernel="radial" )
    #compute the weight vector
    w = t(svmModel$coefs)%*%svmModel$SV
    #compute ranking criteria
    rankingCriteria = w * w
    #rank the features
    ranking = sort(rankingCriteria, index.return = TRUE)$ix
    #update feature ranked list
    featureRankedList[rankedFeatureIndex] = survivingFeaturesIndexes[ranking[1]]
    rankedFeatureIndex = rankedFeatureIndex - 1
    #eliminate the feature with smallest ranking criterion
    (survivingFeaturesIndexes = survivingFeaturesIndexes[-ranking[1]])
  }
  return (featureRankedList)
}
```

SVM-RFE for control vs. pesticide + bullfrog with 75 bins from pest vs. control
```{r}
X<-yuck4[,-76]
Y<-as.factor(yuck4$Class)
#RFE
featureRankedList.yuck4 <-svmrfeFeatureRanking(X,Y)
#### do 250 bootstrapped leave one out cross validation for SVM with 1-75 feature ranked bins ####
no.features<-seq(1,75,by=5)
out.acc<-NULL
for (j in seq_along(no.features)){
  for (i in 1:250){
    svmModel3 = svm(X[, featureRankedList.yuck4[1:no.features[j]]], Y, cost = 1e5, kernel="radial", cross=3 )
    accuracy.vector[i]<-svmModel3$tot.accuracy
  }
  out.acc[j]<-mean(accuracy.vector)  
}
out.acc
acc4merge<-as.data.frame(cbind(no.features, out.acc))
View(acc4merge)
#plot of output
plot(acc4merge$no.features, acc4merge$out.acc, ylab="% Accuracy", xlab="Number of bins")
# between 10 and 20 bins has the highest accuracy so will do the cross validation again to see if there is even higher hidden in there

no.features<-seq(1,20,by=1)
out.acc<-NULL
for (j in seq_along(no.features)){
  for (i in 1:250){
    svmModel3 = svm(X[, featureRankedList.yuck4[1:no.features[j]]], Y, cost = 1e10, kernel="radial", cross=3 )
    accuracy.vector[i]<-svmModel3$tot.accuracy
  }
  out.acc[j]<-mean(accuracy.vector)  
}
out.acc
acc4merge<-as.data.frame(cbind(no.features, out.acc))
View(acc4merge)
#plot of output
plot(acc4merge$no.features, acc4merge$out.acc, ylab="% Accuracy", xlab="Number of bins")

```
With RFE ranked bins highest accuracy classification is %80 with 10 bins when use 75 bins from control vs. pesticide on control vs. pesticide + bullfrog. 

To export spreadsheet with retention time and bin rank for control vs. pesticide + bullfrog but with 75 bins ranked by control vs. pesticide. 
```{r}
library(R.utils)
yuck_exp<-cbind(featureRankedList.yuck4, yuck3)
setwd("~/git/PIP/GCMS_data_XCMS/output")
write.csv(yuck_exp, "RFE_ctrlvspest_bull_75binrank.csv")
```


Now to see how well SVM -rfe will work with just control vs. pesticide + bullfrog.

```{r}
View(allliver2)
liver1_4<-allliver2[which(allliver2$'class$V1'=='1' | allliver2$'class$V1'=='4'), ]
View(liver1_4)
liver1_4$Class<-as.factor(liver1_4$`class$V1`)
set.seed(34)
#test tuning
gamma<-c(0.1,1)
#test tuning to see how long it takes
tuned <- tune.svm(as.factor(Class)~., data = liver1_4, gamma = gamma, cost = 0.1, tunecontrol = tune.control(cross = 3), scale=FALSE)
#tuning across a larger range
cost.vector<-c(1e-10,1e-5,1e-1,1e1,1e5,1e10)
gamma.vector<-c(1e-10,1e-5,1e-1,1e1,1e5,1e10)
accuracy.vector<-NULL
m9<-matrix(NA, length(cost.vector), length(gamma.vector))

for (k in seq_along(cost.vector)){
  for (j in seq_along(gamma.vector)){    
    for (i in 1:250){
      model.radial.all <- svm(as.factor(Class)~., data=liver1_4, kernal="radial", gamma=gamma.vector[j], cost=cost.vector[k], scale=FALSE, cross=3)
      accuracy.vector[i]<-model.radial.all$tot.accuracy
    }
    m9[k, j]<-mean(accuracy.vector)  
  }
}
m9
 [,1]  [,2]  [,3]  [,4]  [,5]  [,6]
[1,] 35.90 36.72 37.20 37.16 36.60 37.32
[2,] 37.18 37.34 37.88 37.04 36.80 36.92
[3,] 36.52 36.50 37.92 36.50 36.72 37.04
[4,] 35.82 36.80 37.20 37.58 37.10 37.26
[5,] 36.14 60.06 36.70 36.94 36.82 36.22
[6,] 50.76 61.04 36.04 36.44 37.24 37.24

```

RFE code optimized tuning for control vs. pesticide + bullfrog all bins 
```{r}
#### other R implementation of RFE for SVM ####
svmrfeFeatureRanking = function(x,y){
  n = ncol(x)
  survivingFeaturesIndexes = seq(1:n)
  featureRankedList = vector(length=n)
  rankedFeatureIndex = n
  while(length(survivingFeaturesIndexes)>0){
    #train the support vector machine
    svmModel = svm(x[, survivingFeaturesIndexes], y, cost = 1e10, gamma=1.0e-5, cachesize=500,
                   scale=F, type="C-classification", kernel="radial" )
    #compute the weight vector
    w = t(svmModel$coefs)%*%svmModel$SV
    #compute ranking criteria
    rankingCriteria = w * w
    #rank the features
    ranking = sort(rankingCriteria, index.return = TRUE)$ix
    #update feature ranked list
    featureRankedList[rankedFeatureIndex] = survivingFeaturesIndexes[ranking[1]]
    rankedFeatureIndex = rankedFeatureIndex - 1
    #eliminate the feature with smallest ranking criterion
    (survivingFeaturesIndexes = survivingFeaturesIndexes[-ranking[1]])
  }
  return (featureRankedList)
}
```
SVM w/ RFE for control vs. pesticide +bullfrog
```{r}
#set up data for RFE
X<-liver1_4[,-1:-2]
X<-X[,-1585]
Y<-liver1_4$Class
#RFE
featureRankedList <-svmrfeFeatureRanking(X,Y)
#SVM w/ RFE test
svmModel = svm(X[, featureRankedList[1:75]], Y, cost = 1e10, gamma=1.0e-10, kernel="radial", cross=3 )
svmModel 
summary(svmModel)
print(svmModel)

no.features<-seq(1,1580,by=100)
out.acc<-NULL
for (j in seq_along(no.features)){
  for (i in 1:250){
    svmModel3 = svm(X[, featureRankedList[1:no.features[j]]], Y, cost = 1e10, kernel="radial", cross=3 )
    accuracy.vector[i]<-svmModel3$tot.accuracy
  }
  out.acc[j]<-mean(accuracy.vector)  
}
out.acc
acc4merge<-as.data.frame(cbind(no.features, out.acc))
View(acc4merge)
#plot of output
plot(acc4merge$no.features, acc4merge$out.acc, ylab="% Accuracy", xlab="Number of bins", main="control vs. pesticide+bullfrog")

no.features<-seq(1,200,by=2)
out.acc<-NULL
for (j in seq_along(no.features)){
  for (i in 1:250){
    svmModel3 = svm(X[, featureRankedList[1:no.features[j]]], Y, cost = 1e10, kernel="radial", cross=3 )
    accuracy.vector[i]<-svmModel3$tot.accuracy
  }
  out.acc[j]<-mean(accuracy.vector)  
}
out.acc
acc4merge<-as.data.frame(cbind(no.features, out.acc))
View(acc4merge)
#plot of output
plot(acc4merge$no.features, acc4merge$out.acc, ylab="% Accuracy", xlab="Number of bins", main="control vs. pesticide+bullfrog")

```
When classifying, control vs. bullfrog+pesticide tadpoles, after 3 rfe bins the classification accuracy and gets to 96% at 9 bins then stays above 90 until 141 bins. 

Combine the feature ranked list with the features for control vs. pesticide+ bullfrog
```{r}
library(R.utils)
liver1_4t<-t(liver1_4)
View(liver1_4t)
#inserted 2 NAs in beginning 
featureRankedList4join <- insert(featureRankedList, ats=1, values=rep(NA,2))
featureRankedList4join2 <- insert(featureRankedList4join, ats=1587, values=(NA))
View(featureRankedList4join2)
dim(liver1_4t)
liver1_4twRFE<-cbind(featureRankedList4join2, liver1_4t)
View(liver1_4twRFE)
setwd("~/git/PIP/GCMS_data_XCMS/output")
write.csv(liver1_4twRFE, "RFE_ctrlvpest_and_bullfrog_allbins.csv")
```
